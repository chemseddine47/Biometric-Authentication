# Biometric Authentication (EyeReg Project)

## üìÖ Context: The State of VR Security (2023)
Back in early 2023, before the current explosion of generative AI took over the tech world, the industry's eyes were fixed on the "Metaverse" and the rapid adoption of XR (Extended Reality) headsets. We saw hardware like the HTC Vive Pro Eye and the anticipation for Apple's entry into the space pushing boundaries. But there was a massive, overlooked flaw: **Identity**.

Once you put on a headset, you are effectively anonymous. Passwords in VR are a nightmare to type, and removing the headset to unlock a phone breaks immersion completely. We needed a security layer that was continuous, invisible, and impossible to fake. That is where **Biometric Eye-Tracking** comes in. The human eye moves in distinct, micro-saccadic patterns that are as unique as a fingerprint. This project was born out of the necessity to secure AR/VR hardware using the very cameras already built into them for foveated rendering.

This repository contains the complete implementation of my comparative study: a "Classic" Signal Processing approach (k-NN) versus a "Modern" Deep Learning approach (CNN).

<p align="center">
  <img src="images/image1.png" width="800">
</p>
---

## üìÇ Project Architecture

The repository is divided into two core engines based on the methodology used.

### 1. `knn_approach/` (The Classic Signal Processing Engine)
This folder contains the implementation of the "Goussem & Djallil" protocol, which relies on manual feature engineering. We treat the eye not just as an image, but as a generator of mathematical signals over time.

* **`processing_utils.py`**
    * **What it does:** The mathematical backbone. It processes video frames to extract the "Eye Aspect Ratio" (EAR).
    * **Input:** Raw video frames.
    * **Processing:** It calculates the vertical and horizontal distances of the eye landmarks to derive the "Quotient" (opening ratio). It also runs a Fast Fourier Transform (FFT) to analyze the frequency of pupil dilation and blinking.
    * **Output:** A numerical vector containing features like `Quotient Min`, `Quotient Max`, `Quotient Mean`, and `FFT Peak`.
* **`knn_engine.py`**
    * **What it does:** The classifier. It uses the k-Nearest Neighbors algorithm to match a live user's signal against the database.
    * **Input:** The feature vector from `processing_utils`.
    * **Output:** The predicted User ID or an "Intruder" alert if the distance threshold is breached.
* **`security_metrics.py`**
    * **What it does:** The auditor. It runs thousands of iterations to calculate the False Acceptance Rate (FAR) and False Rejection Rate (FRR) by adjusting the decision threshold.

### 2. `Cnn_approach/` (The Deep Learning Engine)
This folder represents the evolution of the project. Instead of telling the computer *what* to look for (geometry), we let a Convolutional Neural Network (CNN) learn the spatial features of the eye directly from pixels.

* **`data_gen_panoramic.py`**
    * **What it does:** The extractor. It detects facial landmarks and crops a wide, panoramic image containing **both eyes** simultaneously. This preserves the spatial relationship and symmetry between the left and right eye.
    * **Input:** Raw video files.
    * **Output:** A dataset of 154x102 grayscale images sorted by user.
* **`data_gen_single.py`**
    * **What it does:** Similar to the above, but extracts the Left and Right eyes into separate image files (64x56) to test if we can authenticate using only one eye.
* **`cnn_trainer.py`**
    * **What it does:** The architect. It constructs a CNN with 2 Convolutional layers (ReLU activation), Max Pooling, and 2 Fully Connected Dense layers. It trains the model to classify the 22 distinct users.
    * **Input:** The `.npy` arrays generated by the preprocessor.
    * **Output:** A trained `.h5` model file (the "Brain").
* **`realtime_demo.py`**
    * **What it does:** The proof of concept. It connects to a webcam/video feed, detects eyes in real-time, feeds them to the CNN, and overlays the authentication result on the screen.

---

## üëÅÔ∏è The "EyeReg" Dataset
One of the biggest hurdles in 2023 was the lack of public datasets that focused specifically on the *periocular* region (the area around the eye) for authentication rather than just gaze tracking.

To solve this, I created **EyeReg**.
* **Participants:** 22 unique individuals.
* **Volume:** 220 total videos (10 sessions per participant).
* **Conditions:** Varied lighting (natural vs. artificial) and distances to simulate real-world VR usage.

**‚ö†Ô∏è Privacy Note:** Due to strict privacy agreements signed by the participants, the **raw video files cannot be made public**. However, the code in this repository is fully functional and can be used to extract your own datasets or the anonymized image features if provided.

### Extracted Data Structure
From those videos, the scripts in this repo generated a massive image database for training the CNN:
1.  **Combined Eyes:** ~3,000 images per participant (Total: **69,505 images**).
2.  **Separate Eyes:** ~6,000 images per participant (Total: **138,889 images**).

---
## üî¨ Evaluation Methodology
To ensure a realistic assessment of security, the dataset of 22 participants was strictly divided into two disjoint groups for "Open-Set" Authentication testing:

* **Authorized Group (Clients):** 15 Users (Used for Training and Validation).
* **Impostor Group (Intruders):** 7 Users (Never seen by the model, used solely for Intrusion testing).

#### Metric Calculation
* **FAR (False Acceptance Rate):** Tested by calculating how many of the **7 Impostors** were successfully classified as any of the 15 authorized users.
    * *Formula:* `False Acceptances / Total Impostor Attempts`
* **FRR (False Rejection Rate):** Tested by calculating how many of the **15 Authorized Users** were rejected when trying to access their own accounts.
    * *Formula:* `False Rejections / Total Client Attempts`

*Note: For the CNN approach, a confidence threshold of **99.9%** (0.999) was enforced. Any prediction below this confidence was treated as a rejection.*

---

## üìä Results & Performance
We didn't just build this; we stress-tested it. Using the rigorous split described above, here is how the two methods compared:

### 1. The KNN Baseline
Using the standard geometric features (Quotients + FFT):
* **False Acceptance Rate (FAR):** 14.29% (Security was decent).
* **False Rejection Rate (FRR):** 33.33% (Usability was poor; valid users were rejected too often).

*Optimization:* By increasing the feature set to 9 landmarks (including eye height/width ratios), we managed to drop the **FRR to 26.67%**.

### 2. The CNN Upgrade
The Deep Learning model, trained on the "Combined Eyes" dataset, proved to be superior in balancing security and usability:
* **False Acceptance Rate (FAR):** 14.29% (Matched the strict security of KNN).
* **False Rejection Rate (FRR):** **26.67%** ( Significantly better usability).

### Conclusion

While the math-based k-NN approach is lightweight and explainable, the CNN approach demonstrated that learning the "texture" and "shape" of the eye region provides a much smoother user experience without compromising security.

---
*Authored by Chems Eddine Nemeur, Decembre 2025.*